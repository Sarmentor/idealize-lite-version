#global word count
freqPlot(list(all.text.user$text)) #creating frequency plots
#global word count
freqPlot(list(all.text.user$text)) #creating frequency plots
# Pre-processing of Corpus
makeCorpus <- function(text){ #Function for making corpus and cleaning the tweets fetched
#twitterdf <- do.call("rbind", lapply(text, as.data.frame)) #store the fetched tweets as a data frame
twitterdf <- data.frame(text=text, stringsAsFactors = FALSE)
twitterdf$text <- sapply(twitterdf$text,function(row) iconv(row, "latin1", "ASCII", sub=""))#Removing emoticons from tweets
twitterCorpus <- Corpus(VectorSource(twitterdf$text)) #Creating Corpus
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x)) #function to replace a pattern to white space using regex
twitterCorpus <- tm_map(twitterCorpus, toSpace, "(RT|via)((?:\\b\\W*@\\w+)+)") #match rt or via
twitterCorpus <- tm_map(twitterCorpus, toSpace, "@\\w+") #match @
twitterCorpus <- tm_map(twitterCorpus, toSpace, "[ \t]{2,}") #match tabs
twitterCorpus <- tm_map(twitterCorpus, toSpace, "[ |\n]{1,}") #match new lines
twitterCorpus <- tm_map(twitterCorpus, toSpace, "^ ") #match white space at begenning
twitterCorpus <- tm_map(twitterCorpus, toSpace, " $") #match white space at the end
twitterCorpus <- tm_map(twitterCorpus, PlainTextDocument)
twitterCorpus <- tm_map(twitterCorpus, removeNumbers)
twitterCorpus <- tm_map(twitterCorpus, removePunctuation)
twitterCorpus <- tm_map(twitterCorpus, toSpace, "http[[:alnum:]]*") #remove url from tweets
twitterCorpus <- tm_map(twitterCorpus,removeWords,stopwords("en"))
twitterCorpus <- tm_map(twitterCorpus, content_transformer(tolower))
return(twitterCorpus)
}
#global word count
freqPlot(all.text.user$text) #creating frequency plots
#global word cloud
makeWordcloud(all.text.user$text) #creating wordcloud
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hCluster(costable.dtm.text) #hierarchical clustering
costable.dtm.text
?hcluster
??hcluster
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hClust(costable.dtm.text) #hierarchical clustering
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hCluster(costable.dtm.text) #hierarchical clustering
traceback()
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hCluster.users(costable.dtm.text) #hierarchical clustering
#Clustering
hCluster.users<-function (sim.matrix){ #hierarchical clustering
fit <- hclust(sim.matrix, method="ward.D") #clustering terms
plot(fit)
rect.hclust(fit, k=5) #cutting the tree into 5 clusters
(groups <- cutree(fit, k=5))
}
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hCluster.users(costable.dtm.text) #hierarchical clustering
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hCluster.users(costable.dtm.text) #hierarchical clustering
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hCluster.users(as.matrix(costable.dtm.text)) #hierarchical clustering
traceb ack()
traceback()
hclust(costable.dtm.text, method="ward.D")
?hclust
hclust(as.dist(costable.dtm.text), method="ward.D")
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
hCluster.users(as.dist(costable.dtm.text)) #hierarchical clustering
#fazer cosine similarity por utilizador
#fazer cluster ou rede de similaridade
names(hCluster.users(as.dist(costable.dtm.text))) #hierarchical clustering
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
fit
fit
names(fit)
fit$order
fit$labels
fit$height
fit$merge
fit
groups
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
?dialogWindow
??dialogWindow
??dialog
shiny::runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
traceback()
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
runApp('C:/Users/Rui Sarmento/Dropbox/Projectos/Wikipedia Text-to-Voice/Shiny')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
clust.groups
i=1
names(clust.groups[which(clust.groups==i)])
users.clust <- names(clust.groups[which(clust.groups==i)])
paste(all.text.user[which(all.text.user$doc_id %in% users.clust),"text"])
text.clust <- paste(all.text.user[which(all.text.user$doc_id %in% users.clust),"text"])
tSentimen(text.clust)
tSentimen(paste(text.clust, collapse=" "))
paste(text.clust, collapse=" ")
makeCorpus(text.clust)
unlist(sapply(twicorpus, `[`, "content"))
twicorpus<-makeCorpus(text.clust)
unlist(sapply(twicorpus, `[`, "content"))
names(twicorpus)
twicorpus[[1]]
twicorpus[[1]]$content
twicorpus[[]]$content
twicorpus[[1:2]]$content
twicorpus[[1]]$content
twicorpus[[2]]$content
unlist(twicorpus[[]])$content
unlist(twicorpus)$content
unlist(twicorpus)
class(unlist(twicorpus))
lapply(twicorpus, FUN=function(x){x$content})
lapply(twicorpus, FUN=function(x){x$"content"})
?lapply
twicorpus$content
data.frame(text=twicorpus$content, stringsAsFactors=F)
nrows(data.frame(text=twicorpus$content, stringsAsFactors=F))
nrow(data.frame(text=twicorpus$content, stringsAsFactors=F))
ncol(data.frame(text=twicorpus$content, stringsAsFactors=F))
dataframe<-data.frame(text=twicorpus$content, stringsAsFactors=F)
poldat <- with(dataframe, polarity(text))
names(dataframe)
poldat <- with(dataframe, polarity("text"))
poldat <- with(dataframe, polarity(paste(twicorpus$content,collapse = " ")))
poldat <- polarity(paste(twicorpus$content,collapse = " "))
polarity(paste(twicorpus$content,collapse = " "))
traceback()
`[[.qdap_hash` <- `[[.data.frame`
poldat <- with(dataframe, polarity(text))
tSentimen(text.clust)
text.clust
tSentimen(text.clust)$stan.mean.polarit
tSentimen(text.clust)$stan.mean.polarity
tSentimen(text.clust)["stan.mean.polarity"]
names(tSentimen(text.clust))
tSentimen(text.clust)$all
tSentimen(text.clust)$group
tSentimen(text.clust)$group$stan.mean.polarity
?polarity
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
??rtweet
# Set up authentication
Auth<-create_token(consumer_key=APIKey, consumer_secret=APISecret,access_token = AccessToken, access_secret =AccessTokenSecret)
Auth
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
traceback()
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
remove.packages("twitteR")
library(twitteR)
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
remove.packages(qdap)
remove.packages("qdap")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
install.packages("qdap")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R', encoding = 'UTF-8')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R')
traceback()
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Crypto-News-Sentiment.R')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Bitcoins Stream/Mine/Finance-Twitter-News-Sentiment-Per-Users-Groups.R')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("quantmod")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("tseries")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("caret")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("ipred")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("rio")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages(c("backports", "BH", "callr", "clipr", "colorspace", "curl", "data.table", "dbplyr", "ddalpha", "dynlm", "e1071", "flextable", "forecast", "git2r", "httpuv", "hunspell", "lme4", "openssl", "pdftools", "pillar", "plm", "prabclus", "psych", "purrr", "quanteda", "quantreg", "RcppArmadillo", "RcppParallel", "readr", "readxl", "rlang", "rstudioapi", "slam", "SnowballC", "spacyr", "striprtf", "tibble", "tinytex", "topicmodels", "udpipe", "xgboost"))
install.packages(c("backports", "BH", "callr", "clipr", "colorspace", "curl", "data.table", "dbplyr", "ddalpha", "dynlm", "e1071", "flextable", "forecast", "git2r", "httpuv", "hunspell", "lme4", "openssl", "pdftools", "pillar", "plm", "prabclus", "psych", "purrr", "quanteda", "quantreg", "RcppArmadillo", "RcppParallel", "readr", "readxl", "rlang", "rstudioapi", "slam", "SnowballC", "spacyr", "striprtf", "tibble", "tinytex", "topicmodels", "udpipe", "xgboost"))
install.packages(c("backports", "BH", "callr", "clipr", "colorspace", "curl", "data.table", "dbplyr", "ddalpha", "dynlm", "e1071", "flextable", "forecast", "git2r", "httpuv", "hunspell", "lme4", "openssl", "pdftools", "pillar", "plm", "prabclus", "psych", "purrr", "quanteda", "quantreg", "RcppArmadillo", "RcppParallel", "readr", "readxl", "rlang", "rstudioapi", "slam", "SnowballC", "spacyr", "striprtf", "tibble", "tinytex", "topicmodels", "udpipe", "xgboost"))
install.packages(c("backports", "BH", "callr", "clipr", "colorspace", "curl", "data.table", "dbplyr", "ddalpha", "dynlm", "e1071", "flextable", "forecast", "git2r", "httpuv", "hunspell", "lme4", "openssl", "pdftools", "pillar", "plm", "prabclus", "psych", "purrr", "quanteda", "quantreg", "RcppArmadillo", "RcppParallel", "readr", "readxl", "rlang", "rstudioapi", "slam", "SnowballC", "spacyr", "striprtf", "tibble", "tinytex", "topicmodels", "udpipe", "xgboost"))
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("tibble")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("purrr")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("colorspace")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("data.table")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("xgboost")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
install.packages("ps")
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
url
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
traceback()
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Fundamental Project/Freelancer Code/main_LV.R')
anova(lasso_mod, xgb_mod)
anova(xgb_mod, model)
k = ncol(X_train)
## create your model,and add layers
model <- keras_model_sequential()
model %>%
layer_dense(units = 60, activation = 'relu', input_shape = k) %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 1, activation = 'linear')
summary(model)
model %>% compile(
optimizer = 'rmsprop',
loss = 'mse',
metrics = 'mse'
)
###########################
# Step 2: Train the model #
###########################
model %>% fit(X_train, y_train, epochs=100, batch_size=28, validation_split = 0.1)
################################
# Step 2: Plot the predictions #
################################
pred <- model %>% predict(X_test, batch_size = 28)
print("Next Seven Days Forecast using Neural Network Regression will be : ")
print(pred)
#################################
# Compare the Models with Anova #
#################################
anova(lasso_mod, xgb_mod)
anova(xgb_mod, model)
?anova
model
xgb_mod
lasso_mod
class(lasso_mod)
class(xgb_mod)
class(model)
library(rPython)
install.packages("rPython")
install.packages("Rtools")
install.packages("devtools")
install.packages("Rtools")
library(Rtools)
library(Rtools)
library(Rtools)
library(devtools)
library(rPython)
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/R-CRAN/R/TILES.R')
?python.load
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/R-CRAN/R/TILES.R')
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/R-CRAN/R/TILES.R')
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/R-CRAN/R/TILES.R')
source('C:/Users/Rui Sarmento/Dropbox/Educação/Doutoramento/Tese/Dyncomm R Package/DynComm-R-package/R-CRAN/R/TILES.R')
?py_install
?source_python
inf
?inf
infinity
1/0
Inf
import("networkx")
?nx.Graph()
setwd("C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny")
shiny::runApp()
install.packages("mapdata")
runApp()
install.packages("mapproj")
runApp()
install.packages("BiocInstaller")
source("http://bioconductor.org/biocLite.R")
biocLite("graph")
runApp()
install.packages("gtrendsR")
runApp()
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Trends/Trends.R')
runApp()
runApp()
sub_code
countries[,"country_code"]==country
countries[which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local))),"sub_code"]
countries[which(countries[,"country_code"]=="GB" & countries[,"name"]==paste(toupper(local))),"sub_code"]
loca
local
local=huelva
local <- Huelva
local <- "Huelva"
country = "ES"
countries[which(countries[,"country_code"]=="GB" & countries[,"name"]==paste(toupper(local))),"sub_code"]
countries[which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local))),"sub_code"]
country = "USA"
local <- "Seattle"
countries[which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local))),"sub_code"]
country = "US"
countries[which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local))),"sub_code"]
local <- "Whashigton D.C."
local <- "Whashington D.C."
countries[which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local))),"sub_code"]
country = "US"
countries[which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local))),"sub_code"]
local <- "Washington D.C."
countries[which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local))),"sub_code"]
local <- "Washington"
countries[which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local))),"sub_code"]
local <- "Washington DC"
countries[which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local))),"sub_code"]
countries[which(countries[,"country_code"]==country & countries[,"name"] %in% paste(toupper("Washington"))),"sub_code"]
countries[which(countries[,"country_code"]==country & countries[,"name"] %in% paste(toupper("Washington"))),]
sub_code
caca <- "US-CA"
strsplit("US-CA", sep="-")
strsplit("US-CA", split="-")
runApp()
runApp()
runApp()
sub_code
gtrends(keyword = key, geo = c(strsplit(sub_code, split="-")[2],strsplit(sub_code, split="-")[1]), time = "today 12-m",gprop = c("web", "news", "images", "froogle", "youtube"),category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
gtrends(keyword = key, geo = c(strsplit(sub_code, split="-")[2],strsplit(sub_code, split="-")[1]), time = "today 12-m",gprop = c("web", "news", "images", "froogle", "youtube"),category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
Q
runApp()
?gtrends
runApp()
runApp()
sub_code
gtrends(keyword = key, geo = c("01","PT"), time = "today 12-m",gprop = c("web", "news", "images", "froogle", "youtube"),category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
gtrends(keyword = key, geo = c("01","351"), time = "today 12-m",gprop = c("web", "news", "images", "froogle", "youtube"),category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
Q
source('C:/Users/Rui Sarmento/Dropbox/Projectos/Idealize Shiny/idealize/Shiny/../Trends/Trends.R')
runApp()
sub_code
country
countries[,"country_code"]==country
countries[,"country_code"]=="PT"
names(countries)
length(names(countries))
names(countries)
countries[1:3,]
geo = c(paste(strsplit(sub_code[1], split="-")[2]),paste(strsplit(sub_code[1], split="-")[1]))
geo
geo = c(strsplit(sub_code[1], split="-")[2],strsplit(sub_code[1], split="-")[1])
geo
strsplit(sub_code[1], split="-")[1]
geo = c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1])
geo
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
sub_code
gtrends(keyword = key, geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web", "news","youtube"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
geo
geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1])
geo
gtrends(keyword = key, geo= c("SWA","UK"), time = "today 12-m",gprop = c("web", "news","youtube"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
runApp()
gtrends(keyword = key, geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web", "news","youtube"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
gtrends(keyword = key, geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
runApp()
runApp()
runApp()
?rbind
runApp()
runApp()
trends
keys
key
as.numeric(
gtrends(keyword = key, geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
)
length(as.numeric(
gtrends(keyword = key, geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
))
?as.matrix
runApp()
runApp()
trends
runApp()
runApp()
shiny::runApp()
runApp()
?renderTable
runApp()
runApp()
View(keys.list)
View(keys.list)
View(keys.list["Keywords"])
View(keys.list[,"Keywords"])
keys.list[,"Keywords"]
keys.list[,]
keys.list[,"keywords"]
runApp()
trends
keys
trends <- data.frame()
for(key in 1:length(keys)){
#function from package
trends <- rbind(trends,as.data.frame(t(as.numeric(
gtrends(keyword = keys[key], geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = TRUE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
))))
}
trends
trends
trends
keys
nkeys
runApp()
trends
length(keys)
trends <- data.frame()
for(key in 1:length(keys)){
#function from package
trends <- rbind(trends,as.data.frame(t(as.numeric(
gtrends(keyword = keys[key], geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
))))
}
trends
runApp()
keys
trends
trends
for(key in 1:length(keys)){
#function from package
trends <- rbind(trends,as.data.frame(t(as.numeric(
gtrends(keyword = keys[key], geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
))))
}
trends
runApp()
trends
gtrends(keyword = "orthodox education systems ruin", geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
gtrends(keyword = "childrens emotional", geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
gtrends(keyword = "orthodox education systems ruin", geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
keys
gtrends(keyword = "skilled learners", geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
runApp()
runApp()
keys.list[which(keys.list[,"keywords"]==rownames(trends.table)),"weighted.scores"]
which(keys.list[,"keywords"]==rownames(trends.table))
which(keys.list[,"keywords"]%in%rownames(trends.table))
runApp()
runApp()
local
sub_code
which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local)))
which(countries[,"country_code"]==country)
country
which(countries[,"country_code"]==country)
country ="GB"
which(countries[,"country_code"]==country)
which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local)))
runApp()
runApp()
which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local)))
countries[which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local))),"sub_code"]
gtrends(keyword = keys[key], geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
sub_code <- as.character(countries[which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local))),"sub_code"])
sub_code
sub_code <- as.character(countries[which(countries[,"country_code"]==country & countries[,"name"]==paste(toupper(local))),"sub_code"])
gtrends(keyword = keys[key], geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
key =1
gtrends(keyword = keys[key], geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1])
c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1])[2]=="EN"
c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1])[2]<-"EN"
c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1])[2] -> caca
caca[2]<- "EN"
gtrends(keyword = keys[key], geo= caca, time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
caca
c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]) -> caca
gtrends(keyword = keys[key], geo= caca, time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
caca[2]<- "EN"
gtrends(keyword = keys[key], geo= caca, time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
Q
runApp()
sub_code
gtrends(keyword = keys[key], geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1]), time = "today 12-m",gprop = c("web"), category = 0, hl = "en-US", low_search_volume = FALSE,cookie_url = "http://trends.google.com/Cookies/NID")$interest_over_time$hits
geo= c(strsplit(sub_code[1], split="-")[[1]][2],strsplit(sub_code[1], split="-")[[1]][1])
geo
runApp()
data(countries)
summary(data(countries))
countries
geo= c(strsplit(sub_code[1], split="-")[[1]][2],ifelse(strsplit(sub_code[1], split="-")[[1]][1]=="GB",paste("EN"),paste(strsplit(sub_code[1], split="-")[[1]][1])))
?gtrends
runApp()
